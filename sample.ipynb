{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REQUIREMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INSTALLATION GUIDE\n",
    "\n",
    "##### support matrix\n",
    "https://nvidia.github.io/TensorRT-LLM/reference/support-matrix.html\n",
    "\n",
    "##### installations\n",
    "https://nvidia.github.io/TensorRT-LLM/installation/linux.html\n",
    "\n",
    "https://nvidia.github.io/TensorRT-LLM/installation/build-from-source-linux.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain and start the basic docker image environment (optional).\n",
    "!docker run --rm --runtime=nvidia --gpus all --entrypoint /bin/bash -it nvidia/cuda:12.1.0-devel-ubuntu22.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies, TensorRT-LLM requires Python 3.10\n",
    "!apt-get update && apt-get -y install python3.10 python3-pip openmpi-bin libopenmpi-dev git git-lfs\n",
    "\n",
    "# Install the latest preview version (corresponding to the main branch) of TensorRT-LLM.\n",
    "# If you want to install the stable version (corresponding to the release branch), please\n",
    "# remove the `--pre` option.\n",
    "# use tensorrt_llm = 0.10.0\n",
    "!pip3 install tensorrt_llm -U  --extra-index-url https://pypi.nvidia.com\n",
    "\n",
    "# Check installation\n",
    "!python3 -c \"import tensorrt_llm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### download checkpoint from nvidia NGC catlog  \n",
    "##### in this sample we will be making use of mistral 7B\n",
    "##### but this API supports mistral,phi3,llama2 and llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dowmload the model from nvidia NGC catalog\n",
    "!wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/llama/mistral-7b-int4-chat/versions/1.2/zip -O mistral-7b-int4-chat_1.2.zip\n",
    "\n",
    "# unzip the model \n",
    "\n",
    "# clone the nvidia tensorrt llm repo\n",
    "!git clone -b v0.10.0 https://github.com/NVIDIA/TensorRT-LLM.git\n",
    "\n",
    "# change directory to the build folder for each model\n",
    "!cd TensorRT-LLM\n",
    "!cd examples/llama\n",
    "\n",
    "\n",
    "# build option 1\n",
    "!trtllm-build --checkpoint_dir  #directory to the downloaded check point \\\n",
    "              --output_dir      # directory to file path of build engine  \\\n",
    "              --gemm_plugin float16 \n",
    "\n",
    "# build option 2\n",
    "# build with streaming LLM\n",
    "!trtllm-build --checkpoint_dir  #directory to the downloaded check point \\\n",
    "              --output_dir      # directory to file path of build engine  \\\n",
    "              --gemm_plugin float16 \\\n",
    "              --streamingllm enable\n",
    "\n",
    "# build option 3\n",
    "!trtllm-build --checkpoint_dir  #directory to the downloaded check point \\\n",
    "              --output_dir      # directory to file path of build engine  \\\n",
    "              --gemm_plugin float16 \\\n",
    "              --streamingllm enable \\\n",
    "              --max_batch_size 8 \\\n",
    "              --max_input_len 1024 \\\n",
    "              --max_output_len 1024 \\   \n",
    "              --tp_size 1 \\\n",
    "              --pp_size 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFERENCING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_api import TRTLLM_API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create an instance of the inference API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##### replace this with the path of your build trt engine and tokenizer paths of the particular model \n",
    "engine = \"/workspace/trt-llmv10/models/llama3-int4-AWQ/engine\"\n",
    "token_dr = \"/workspace/trt-llmv10/models/llama3-int4-AWQ/tokenizer\"\n",
    "\n",
    "# create an instance of the model \n",
    "inference_api = TRTLLM_API(engine_dir=engine,token_dir=token_dr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### generate the response of your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_api.generate(input_text= \"write a simple python code \",  # input promp\n",
    "                       streaming=True,                            # streaming output \n",
    "                       temperature=1,                             # temperature \n",
    "                       max_output_len=500,                        # maximum token output \n",
    "                       streaming_interval=1                       # streaming interval if streaming is true\n",
    "                       )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
